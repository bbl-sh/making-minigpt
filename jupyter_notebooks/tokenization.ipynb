{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27a3bcf-316f-4aac-8607-a81d7f9c181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df3d246-096d-4d4d-b12d-99efb71ad8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e48687-bd65-491b-8664-1f29718abf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602d951b-24bc-43bf-b782-fc19b755d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd7e5df-3ce4-4467-880e-8394bd037329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trying',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'implement',\n",
       " ' ',\n",
       " 'tokenizer.',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'am',\n",
       " ' ',\n",
       " 'also',\n",
       " ' ',\n",
       " 'learning',\n",
       " ' ',\n",
       " 'russian.Da']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"Trying to implement tokenizer. I am also learning russian.Da\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be05523-5bf3-479b-8d4b-a98c0660495e",
   "metadata": {},
   "source": [
    "### regular expression to remove ,. and new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf79362-a775-4ed7-9fa7-5c802ad24ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trying',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'implement',\n",
       " ' ',\n",
       " 'tokenizer',\n",
       " '.',\n",
       " '',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'am',\n",
       " ' ',\n",
       " 'also',\n",
       " ' ',\n",
       " 'learning',\n",
       " ' ',\n",
       " 'russian',\n",
       " '.',\n",
       " 'Da']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0dd90-0570-46a5-b8d8-0685f40432b4",
   "metadata": {},
   "source": [
    "### Removing the whitespaces, colon, semicolon, questionmark etc too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b13c9a4-8dcb-4f7a-9f6c-6fdb454feab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trying',\n",
       " 'to',\n",
       " 'implement',\n",
       " 'tokenizer',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'also',\n",
       " 'learning',\n",
       " 'russian',\n",
       " '.',\n",
       " 'Da']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f92f7035-3aa7-420d-bffa-c9b96d857bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Trying', 0)\n",
      "('to', 1)\n",
      "('implement', 2)\n",
      "('tokenizer', 3)\n",
      "('.', 10)\n",
      "('I', 5)\n",
      "('am', 6)\n",
      "('also', 7)\n",
      "('learning', 8)\n",
      "('russian', 9)\n",
      "('Da', 11)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: i for i, token in enumerate(preprocessed)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16ee44-9f8f-4a27-b9f3-76365022812e",
   "metadata": {},
   "source": [
    "## Implementing simple tokeinizer with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d67ffcd8-de00-4410-a61d-5cc5c0e057db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpletokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        encoded_val =  [self.str_to_int[s] for s in preprocessed]\n",
    "        return encoded_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32d32706-ace6-4c6a-bc7b-45bac449ea4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'i'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m simpletokenizer(vocab)\n\u001b[1;32m      2\u001b[0m text1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi am\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m val\n",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36msimpletokenizer.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      6\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 8\u001b[0m encoded_val \u001b[38;5;241m=\u001b[39m  [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_val\n",
      "\u001b[0;31mKeyError\u001b[0m: 'i'"
     ]
    }
   ],
   "source": [
    "tokenizer = simpletokenizer(vocab)\n",
    "text1 = \"i am\"\n",
    "val = tokenizer.encode(text1)\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d828a6-3180-4ff0-a387-52f6ddc9fd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
